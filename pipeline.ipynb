{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f36ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (3.9.1)\n",
      "Requirement already satisfied: jellyfish in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (0.8.3)\n",
      "Requirement already satisfied: click in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\navu2\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk jellyfish pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\navu2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\navu2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\navu2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing file: Adobe.txt\n",
      "\n",
      " Original text snippet: what is adobe?\n",
      "\n",
      "The company was founded in 1982 by John Warn ...\n",
      "\n",
      " Processing file: Amazon.txt\n",
      "\n",
      " Original text snippet: What is amazon?\n",
      "\n",
      "Amazon.com, online retailer, manufacturer o ...\n",
      "\n",
      " Processing file: apple.txt\n",
      "\n",
      " Original text snippet: what is apple?\n",
      "\n",
      "Apple Inc. is an American multinational tech ...\n",
      "\n",
      " Processing file: Binance.txt\n",
      "\n",
      " Original text snippet: What is binance?\n",
      "\n",
      "The Binance Exchange is a leading cryptocu ...\n",
      "\n",
      " Processing file: bing.txt\n",
      "\n",
      " Original text snippet: What Is Bing and How to Use It\n",
      "Google isn't the only search  ...\n",
      "\n",
      " Processing file: blackberry.txt\n",
      "\n",
      " Original text snippet: what is blackberry?\n",
      "\n",
      "BlackBerry is a brand of smartphones, t ...\n",
      "\n",
      " Processing file: canva.txt\n",
      "\n",
      " Original text snippet: What is Canva? A guide to the graphic design platform's feat ...\n",
      "\n",
      " Processing file: Dell.txt\n",
      "\n",
      " Original text snippet: What is dell?\n",
      "\n",
      "The company, first named PC’s Limited, was fo ...\n",
      "\n",
      " Processing file: Discord.txt\n",
      "\n",
      " Original text snippet: What is discord?\n",
      "\n",
      "Discord is a VoIP, instant messaging and d ...\n",
      "\n",
      " Processing file: flipkart.txt\n",
      "\n",
      " Original text snippet: What is flipkart?\n",
      "\n",
      "On May 9, 2018, Walmart announced that it ...\n",
      "\n",
      " Processing file: google.txt\n",
      "\n",
      " Original text snippet: Originally known as BackRub. Google is a search engine that  ...\n",
      "\n",
      " Processing file: HP.txt\n",
      "\n",
      " Original text snippet: What is hp?\n",
      "\n",
      "Hewlett-Packard (HP) was founded in 1939 by Wil ...\n",
      "\n",
      " Processing file: huawei.txt\n",
      "\n",
      " Original text snippet: what is huawei?\n",
      "\n",
      "Huawei Technologies Co., Ltd. is a Chinese  ...\n",
      "\n",
      " Processing file: instagram.txt\n",
      "\n",
      " Original text snippet: What is instagram?\n",
      "\n",
      "With over a billion registered accounts, ...\n",
      "\n",
      " Processing file: Lenovo.txt\n",
      "\n",
      " Original text snippet: What is Lenovo?\n",
      "\n",
      "Lenovo is an international technology compa ...\n",
      "\n",
      " Processing file: levis.txt\n",
      "\n",
      " Original text snippet: what is levis?\n",
      "\n",
      "Walter A. Haas, (born May 11, 1889, San Fran ...\n",
      "\n",
      " Processing file: messenger.txt\n",
      "\n",
      " Original text snippet: what is messenger?\n",
      "\n",
      "Users simply download the app to their m ...\n",
      "\n",
      " Processing file: microsoft.txt\n",
      "\n",
      " Original text snippet: what is microsoft?\n",
      "\n",
      "Microsoft Corporation is an American mul ...\n",
      "\n",
      " Processing file: motorola.txt\n",
      "\n",
      " Original text snippet: what is motorola?\n",
      "\n",
      "Motorola, Inc. was an American multinatio ...\n",
      "\n",
      " Processing file: nike.txt\n",
      "\n",
      " Original text snippet: What is nike?\n",
      "\n",
      "Nike is a champion brand builder. Its adverti ...\n",
      "\n",
      " Processing file: nokia.txt\n",
      "\n",
      " Original text snippet: what is nokia?\n",
      "\n",
      "Nokia Corporation is a Finnish multinational ...\n",
      "\n",
      " Processing file: Ola.txt\n",
      "\n",
      " Original text snippet: What is Ola?\n",
      "\n",
      "Ola needs no introduction. The first Indian ca ...\n",
      "\n",
      " Processing file: operating.txt\n",
      "\n",
      " Original text snippet: What is operating system?\n",
      "\n",
      "An operating system is the most i ...\n",
      "\n",
      " Processing file: paypal.txt\n",
      "\n",
      " Original text snippet: what is paypal?\n",
      "\n",
      "paypal, American e-commerce company formed  ...\n",
      "\n",
      " Processing file: puma.txt\n",
      "\n",
      " Original text snippet: What is puma?\n",
      "\n",
      "Puma SE is engaged in footwear, apparel, and  ...\n",
      "\n",
      " Processing file: reddit.txt\n",
      "\n",
      " Original text snippet: What is reddit?\n",
      "\n",
      "If you spend a lot of time online, chances  ...\n",
      "\n",
      " Processing file: reliance.txt\n",
      "\n",
      " Original text snippet: what is reliance?\n",
      "\n",
      "Mukesh ambani, in full Mukesh Dhirubhai a ...\n",
      "\n",
      " Processing file: samsung.txt\n",
      "\n",
      " Original text snippet: what is samsung?\n",
      "\n",
      "Samsung, South Korean company that is one  ...\n",
      "\n",
      " Processing file: shakespeare.txt\n",
      "\n",
      " Original text snippet: what is shakespeare?\n",
      "\n",
      "William shakespeare, shakespeare also  ...\n",
      "\n",
      " Processing file: skype.txt\n",
      "\n",
      " Original text snippet: What is skype?\n",
      "\n",
      "Skype is a VoIP service that enables people  ...\n",
      "\n",
      " Processing file: sony.txt\n",
      "\n",
      " Original text snippet: what is sony?\n",
      "\n",
      "Sony, in full Sony Corporation, major Japanes ...\n",
      "\n",
      " Processing file: spotify.txt\n",
      "\n",
      " Original text snippet: what is spotify?\n",
      "\n",
      "Sean Parker, (born December 3, 1979), Amer ...\n",
      "\n",
      " Processing file: steam.txt\n",
      "\n",
      " Original text snippet: What is steam?\n",
      "\n",
      "Steam is a video game digital distribution s ...\n",
      "\n",
      " Processing file: swiggy.txt\n",
      "\n",
      " Original text snippet: What Is Swiggy And How It’s Working?\n",
      "\n",
      "Swiggy is one of the t ...\n",
      "\n",
      " Processing file: telegram.txt\n",
      "\n",
      " Original text snippet: what is telegram?\n",
      "\n",
      "Telegram is a popular cross-platform mess ...\n",
      "\n",
      " Processing file: Uber.txt\n",
      "\n",
      " Original text snippet: All about Uber?\n",
      "\n",
      "From that initial launch in San Francisco,  ...\n",
      "\n",
      " Processing file: volkswagen.txt\n",
      "\n",
      " Original text snippet: what is volkswagen?\n",
      "\n",
      "Volkswagen Group, also called Volkswage ...\n",
      "\n",
      " Processing file: whatsapp.txt\n",
      "\n",
      " Original text snippet: What is whatsapp?\n",
      "\n",
      "Launched in 2009, whatsapp is one of the  ...\n",
      "\n",
      " Processing file: yahoo.txt\n",
      "\n",
      " Original text snippet: What Is Yahoo?\n",
      "Everything you can do on Yahoo.com\n",
      "by Tim Fis ...\n",
      "\n",
      " Processing file: youtube.txt\n",
      "\n",
      " Original text snippet: What is youtube?\n",
      "\n",
      "YouTube is a free video sharing website th ...\n",
      "\n",
      " Processing file: zomato.txt\n",
      "\n",
      " Original text snippet: If you are a restaurant owner or marketing manager for a res ...\n",
      "original query: enviroment protecion\n",
      "corrected query: ['environment', 'protection']\n"
     ]
    }
   ],
   "source": [
    "# TEXT PREPROCESSING PIPELINE\n",
    " \n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import jellyfish\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# download nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    print(\"\\n Original text snippet:\", text[:60], \"...\")\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered = [w for w in tokens if w.isalnum() and w not in stop_words]\n",
    "\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(w) for w in filtered]\n",
    "\n",
    "    # soundex encoding\n",
    "    soundex_codes = [jellyfish.soundex(w) for w in stemmed]\n",
    "\n",
    "    return stemmed, soundex_codes\n",
    "\n",
    "def read_corpus(folder_path):\n",
    "    corpus = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):   # only text files\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            print(f\"\\n Processing file: {filename}\")\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                processed, soundexed = preprocess_text(text)\n",
    "                corpus[filename] = {\n",
    "                    \"original\": text,\n",
    "                    \"processed\": processed,\n",
    "                    \"soundex\": soundexed\n",
    "                }\n",
    "    return corpus\n",
    "\n",
    "\n",
    "#SPELLING CORRECTION\n",
    "\n",
    "def correct_query(query, vocab):\n",
    "    spell = SpellChecker()\n",
    "    tokens = nltk.word_tokenize(query.lower())\n",
    "    corrected = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word not in vocab:  \n",
    "            suggestion = spell.correction(word)\n",
    "            if suggestion is None:  # fallback using levenshtein distance\n",
    "                # find word in vocab with min distance\n",
    "                min_dist = float(\"inf\")\n",
    "                best = word\n",
    "                for v in vocab:\n",
    "                    d = jellyfish.levenshtein_distance(word, v)\n",
    "                    if d < min_dist:\n",
    "                        min_dist = d\n",
    "                        best = v\n",
    "                corrected.append(best)\n",
    "            else:\n",
    "                corrected.append(suggestion)\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return corrected\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"Corpus\"  # folder name here\n",
    "    data = read_corpus(folder)\n",
    "\n",
    "    # build vocab from all processed words\n",
    "    vocab = set()\n",
    "    for doc in data.values():\n",
    "        vocab.update(doc[\"processed\"])\n",
    "\n",
    "    # testing query correction\n",
    "    query = \"enviroment protecion\"\n",
    "    print(\"original query:\", query)\n",
    "    print(\"corrected query:\", correct_query(query, vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78640ebd-0088-40e1-99b0-8d30157a322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building inverted index from corpus data...\n",
      "\n",
      "=== BUILDING INVERTED INDEX ===\n",
      "Processing Adobe.txt: 309 unique terms\n",
      "Processing Amazon.txt: 292 unique terms\n",
      "Processing apple.txt: 225 unique terms\n",
      "Processing Binance.txt: 244 unique terms\n",
      "Processing bing.txt: 262 unique terms\n",
      "Processing blackberry.txt: 344 unique terms\n",
      "Processing canva.txt: 185 unique terms\n",
      "Processing Dell.txt: 405 unique terms\n",
      "Processing Discord.txt: 244 unique terms\n",
      "Processing flipkart.txt: 238 unique terms\n",
      "Processing google.txt: 511 unique terms\n",
      "Processing HP.txt: 282 unique terms\n",
      "Processing huawei.txt: 271 unique terms\n",
      "Processing instagram.txt: 247 unique terms\n",
      "Processing Lenovo.txt: 237 unique terms\n",
      "Processing levis.txt: 219 unique terms\n",
      "Processing messenger.txt: 285 unique terms\n",
      "Processing microsoft.txt: 358 unique terms\n",
      "Processing motorola.txt: 348 unique terms\n",
      "Processing nike.txt: 370 unique terms\n",
      "Processing nokia.txt: 242 unique terms\n",
      "Processing Ola.txt: 375 unique terms\n",
      "Processing operating.txt: 243 unique terms\n",
      "Processing paypal.txt: 240 unique terms\n",
      "Processing puma.txt: 233 unique terms\n",
      "Processing reddit.txt: 365 unique terms\n",
      "Processing reliance.txt: 309 unique terms\n",
      "Processing samsung.txt: 192 unique terms\n",
      "Processing shakespeare.txt: 369 unique terms\n",
      "Processing skype.txt: 208 unique terms\n",
      "Processing sony.txt: 390 unique terms\n",
      "Processing spotify.txt: 222 unique terms\n",
      "Processing steam.txt: 259 unique terms\n",
      "Processing swiggy.txt: 190 unique terms\n",
      "Processing telegram.txt: 236 unique terms\n",
      "Processing Uber.txt: 270 unique terms\n",
      "Processing volkswagen.txt: 258 unique terms\n",
      "Processing whatsapp.txt: 317 unique terms\n",
      "Processing yahoo.txt: 189 unique terms\n",
      "Processing youtube.txt: 287 unique terms\n",
      "Processing zomato.txt: 433 unique terms\n",
      "Index built with 3966 unique terms\n",
      "\n",
      "=== INDEX STATISTICS ===\n",
      "Total unique terms: 3966\n",
      "\n",
      "Top 10 terms by document frequency:\n",
      "  also: appears in 35 documents\n",
      "  one: appears in 34 documents\n",
      "  compani: appears in 30 documents\n",
      "  servic: appears in 30 documents\n",
      "  busi: appears in 29 documents\n",
      "  market: appears in 28 documents\n",
      "  includ: appears in 28 documents\n",
      "  new: appears in 28 documents\n",
      "  world: appears in 28 documents\n",
      "  use: appears in 28 documents\n",
      "\n",
      "Sample postings (first 3 terms):\n",
      "  adob:\n",
      "    df: 1\n",
      "    postings: [('Adobe.txt', 21)]\n",
      "  compani:\n",
      "    df: 30\n",
      "    postings: [('Adobe.txt', 6), ('Amazon.txt', 15), ('apple.txt', 10), ('Binance.txt', 2), ('blackberry.txt', 3), ('Dell.txt', 19), ('flipkart.txt', 6), ('google.txt', 4), ('HP.txt', 8), ('huawei.txt', 9), ('Lenovo.txt', 9), ('levis.txt', 9), ('messenger.txt', 1), ('microsoft.txt', 12), ('motorola.txt', 23), ('nike.txt', 13), ('nokia.txt', 18), ('Ola.txt', 7), ('operating.txt', 1), ('paypal.txt', 7), ('puma.txt', 15), ('reliance.txt', 6), ('samsung.txt', 11), ('sony.txt', 20), ('spotify.txt', 3), ('telegram.txt', 2), ('Uber.txt', 8), ('volkswagen.txt', 8), ('yahoo.txt', 1), ('youtube.txt', 3)]\n",
      "  found:\n",
      "    df: 22\n",
      "    postings: [('Adobe.txt', 1), ('apple.txt', 5), ('Binance.txt', 2), ('blackberry.txt', 2), ('Dell.txt', 3), ('HP.txt', 2), ('huawei.txt', 1), ('Lenovo.txt', 2), ('microsoft.txt', 3), ('motorola.txt', 2), ('nokia.txt', 3), ('puma.txt', 3), ('reliance.txt', 1), ('samsung.txt', 1), ('shakespeare.txt', 1), ('spotify.txt', 3), ('steam.txt', 1), ('telegram.txt', 1), ('Uber.txt', 1), ('volkswagen.txt', 2), ('youtube.txt', 1), ('zomato.txt', 1)]\n",
      "\n",
      "=== SAVING INDEX TO inverted_index.json ===\n",
      "Index successfully saved to inverted_index.json\n",
      "File size: 841.69 KB\n",
      "\n",
      "=== SAVING INDEX TO inverted_index.pkl (PICKLE) ===\n",
      "Index successfully saved to inverted_index.pkl\n",
      "File size: 156.54 KB\n",
      "\n",
      "==================================================\n",
      "TESTING LOAD FUNCTIONALITY\n",
      "\n",
      "=== LOADING INDEX FROM inverted_index.json ===\n",
      "Index successfully loaded from inverted_index.json\n",
      "Loaded 3966 terms\n",
      "Loaded index has 3966 terms\n",
      "\n",
      "==================================================\n",
      "TESTING SEARCH FUNCTIONALITY\n",
      "\n",
      "Term: 'environ'\n",
      "Document frequency: 4\n",
      "Postings: [('HP.txt', 3), ('nike.txt', 1), ('operating.txt', 1), ('steam.txt', 1)]\n",
      "\n",
      "Term: 'protect'\n",
      "Document frequency: 5\n",
      "Postings: [('google.txt', 1), ('paypal.txt', 1), ('telegram.txt', 1), ('volkswagen.txt', 1), ('youtube.txt', 1)]\n",
      "\n",
      "Term: 'research'\n",
      "Document frequency: 11\n",
      "Postings: [('Adobe.txt', 1), ('Amazon.txt', 1), ('bing.txt', 1), ('blackberry.txt', 1), ('Dell.txt', 1), ('google.txt', 1), ('huawei.txt', 1), ('microsoft.txt', 2), ('Ola.txt', 2), ('samsung.txt', 1), ('steam.txt', 3)]\n"
     ]
    }
   ],
   "source": [
    "# INVERTED INDEX IMPLEMENTATION\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def build_inverted_index(corpus_data):\n",
    "    print(\"\\n=== BUILDING INVERTED INDEX ===\")\n",
    "    \n",
    "    # initialize the inverted index\n",
    "    inverted_index = defaultdict(lambda: {\"df\": 0, \"postings\": []})\n",
    "    \n",
    "    # process each document\n",
    "    for doc_id, doc_info in corpus_data.items():\n",
    "        processed_terms = doc_info[\"processed\"]  # Get stemmed tokens\n",
    "        \n",
    "        term_freq = Counter(processed_terms)\n",
    "        \n",
    "        print(f\"Processing {doc_id}: {len(term_freq)} unique terms\")\n",
    "        \n",
    "        # Add to inverted index\n",
    "        for term, freq in term_freq.items():\n",
    "            inverted_index[term][\"postings\"].append((doc_id, freq))\n",
    "            inverted_index[term][\"df\"] += 1\n",
    "    \n",
    "    # Convert defaultdict to regular dict for cleaner output\n",
    "    inverted_index = dict(inverted_index)\n",
    "    \n",
    "    print(f\"Index built with {len(inverted_index)} unique terms\")\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def display_index_stats(inverted_index):\n",
    "    \"\"\"Display statistics about the inverted index.\"\"\"\n",
    "    print(\"\\n=== INDEX STATISTICS ===\")\n",
    "    print(f\"Total unique terms: {len(inverted_index)}\")\n",
    "    \n",
    "    # Find terms with highest document frequency\n",
    "    top_terms = sorted(inverted_index.items(), \n",
    "                      key=lambda x: x[1][\"df\"], \n",
    "                      reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nTop 10 terms by document frequency:\")\n",
    "    for term, info in top_terms:\n",
    "        print(f\"  {term}: appears in {info['df']} documents\")\n",
    "    \n",
    "    # Sample postings for first few terms\n",
    "    print(\"\\nSample postings (first 3 terms):\")\n",
    "    for i, (term, info) in enumerate(list(inverted_index.items())[:3]):\n",
    "        print(f\"  {term}:\")\n",
    "        print(f\"    df: {info['df']}\")\n",
    "        print(f\"    postings: {info['postings']}\")\n",
    "\n",
    "\n",
    "def save_index_json(inverted_index, filename=\"inverted_index.json\"):\n",
    "    print(f\"\\n=== SAVING INDEX TO {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(inverted_index, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Index successfully saved to {filename}\")\n",
    "        \n",
    "        # Display file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"File size: {file_size:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving index: {e}\")\n",
    "\n",
    "\n",
    "def load_index_json(filename=\"inverted_index.json\"):\n",
    "    print(f\"\\n=== LOADING INDEX FROM {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            inverted_index = json.load(f)\n",
    "        \n",
    "        print(f\"Index successfully loaded from {filename}\")\n",
    "        print(f\"Loaded {len(inverted_index)} terms\")\n",
    "        return inverted_index\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_index_pickle(inverted_index, filename=\"inverted_index.pkl\"):\n",
    "    print(f\"\\n=== SAVING INDEX TO {filename} (PICKLE) ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(inverted_index, f)\n",
    "        print(f\"Index successfully saved to {filename}\")\n",
    "        \n",
    "        # Display file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"File size: {file_size:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving index: {e}\")\n",
    "\n",
    "\n",
    "def load_index_pickle(filename=\"inverted_index.pkl\"):\n",
    "    print(f\"\\n=== LOADING INDEX FROM {filename} (PICKLE) ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            inverted_index = pickle.load(f)\n",
    "        \n",
    "        print(f\"Index successfully loaded from {filename}\")\n",
    "        print(f\"Loaded {len(inverted_index)} terms\")\n",
    "        return inverted_index\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_term_in_index(inverted_index, term):\n",
    "    if term in inverted_index:\n",
    "        info = inverted_index[term]\n",
    "        print(f\"\\nTerm: '{term}'\")\n",
    "        print(f\"Document frequency: {info['df']}\")\n",
    "        print(f\"Postings: {info['postings']}\")\n",
    "    else:\n",
    "        print(f\"\\nTerm '{term}' not found in index\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Build the inverted index\n",
    "    print(\"Building inverted index from corpus data...\")\n",
    "    inverted_index = build_inverted_index(data)  # 'data' from preprocessing step\n",
    "    \n",
    "    # Display statistics\n",
    "    display_index_stats(inverted_index)\n",
    "    \n",
    "    # Save the index (both formats)\n",
    "    save_index_json(inverted_index)\n",
    "    save_index_pickle(inverted_index)\n",
    "    \n",
    "    # Test loading\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING LOAD FUNCTIONALITY\")\n",
    "    \n",
    "    loaded_index = load_index_json()\n",
    "    if loaded_index:\n",
    "        print(f\"Loaded index has {len(loaded_index)} terms\")\n",
    "    \n",
    "    # Test search functionality\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING SEARCH FUNCTIONALITY\")\n",
    "    \n",
    "    test_terms = [\"environ\", \"protect\", \"research\"] \n",
    "    for term in test_terms:\n",
    "        search_term_in_index(inverted_index, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48406c1a-bec2-4b79-befb-85f7d643d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector lengths saved to doc_lengths.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "def load_index_json(filename=\"inverted_index.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def compute_doc_vector_lengths(inverted_index):\n",
    "    \n",
    "    doc_vectors = {}  \n",
    "    for term, info in inverted_index.items():\n",
    "        for doc_id, tf in info['postings']:\n",
    "            if doc_id not in doc_vectors:\n",
    "                doc_vectors[doc_id] = {}\n",
    "            weight = 1 + math.log10(tf) if tf > 0 else 0\n",
    "            doc_vectors[doc_id][term] = weight\n",
    "\n",
    "    doc_lengths = {}\n",
    "    for doc_id, vec in doc_vectors.items():\n",
    "        length = math.sqrt(sum(w**2 for w in vec.values()))\n",
    "        doc_lengths[doc_id] = length\n",
    "    return doc_lengths\n",
    "\n",
    "def save_doc_lengths(doc_lengths, filename=\"doc_lengths.json\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(doc_lengths, f, indent=2)\n",
    "\n",
    "inverted_index = load_index_json(\"inverted_index.json\")\n",
    "doc_lengths = compute_doc_vector_lengths(inverted_index)\n",
    "save_doc_lengths(doc_lengths, \"doc_lengths.json\")\n",
    "print(\"Document vector lengths saved to doc_lengths.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484e1560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user query: The company is headquartered in Mountaine View, Californea\n",
      "Corrected query: the company is headquartered in mountain view , california\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Top 10 Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[google.txt](corpus\\google.txt)** — Score: `0.0729`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Adobe.txt](corpus\\Adobe.txt)** — Score: `0.0494`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[Discord.txt](corpus\\Discord.txt)** — Score: `0.0379`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[yahoo.txt](corpus\\yahoo.txt)** — Score: `0.0354`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[apple.txt](corpus\\apple.txt)** — Score: `0.0330`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[reddit.txt](corpus\\reddit.txt)** — Score: `0.0308`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[HP.txt](corpus\\HP.txt)** — Score: `0.0307`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[youtube.txt](corpus\\youtube.txt)** — Score: `0.0294`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[whatsapp.txt](corpus\\whatsapp.txt)** — Score: `0.0246`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**[zomato.txt](corpus\\zomato.txt)** — Score: `0.0227`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "# --- Helper functions ---\n",
    "def preprocess_query(query):\n",
    "    if isinstance(query, list):\n",
    "        tokens = query\n",
    "    else:\n",
    "        tokens, _ = preprocess_text(query)   # uses earlier preprocessing pipeline\n",
    "    return tokens\n",
    "\n",
    "def load_index_json(filename=\"inverted_index.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_doc_lengths(filename=\"doc_lengths.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def build_vocab_from_index(inverted_index):\n",
    "    return set(inverted_index.keys())\n",
    "\n",
    "def search_vsm(query, inverted_index, doc_lengths, top_k=10):\n",
    "    N = len(doc_lengths)\n",
    "    query_terms = preprocess_query(query)\n",
    "    tf_query = Counter(query_terms)\n",
    "\n",
    "    query_weights = {}\n",
    "    for term, tf in tf_query.items():\n",
    "        if term in inverted_index:\n",
    "            df = inverted_index[term]['df']\n",
    "            idf = math.log10(N / df) if df > 0 else 0\n",
    "            weight = (1 + math.log10(tf)) * idf\n",
    "            query_weights[term] = weight\n",
    "        else:\n",
    "            query_weights[term] = 0\n",
    "\n",
    "    qlen = math.sqrt(sum(w**2 for w in query_weights.values()))\n",
    "    if qlen > 0:\n",
    "        for term in query_weights:\n",
    "            query_weights[term] /= qlen\n",
    "\n",
    "    scores = {}\n",
    "    for term, q_weight in query_weights.items():\n",
    "        if term in inverted_index:\n",
    "            for doc_id, tf in inverted_index[term]['postings']:\n",
    "                d_weight = 1 + math.log10(tf) if tf > 0 else 0\n",
    "                d_weight /= doc_lengths[doc_id] if doc_lengths[doc_id] > 0 else 1\n",
    "                scores[doc_id] = scores.get(doc_id, 0) + q_weight * d_weight\n",
    "\n",
    "    ranked = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# --- Load index and lengths ---\n",
    "inverted_index = load_index_json(\"inverted_index.json\")\n",
    "doc_lengths = load_doc_lengths(\"doc_lengths.json\")\n",
    "vocab = build_vocab_from_index(inverted_index)\n",
    "\n",
    "# Directly map doc_id to actual corpus file path\n",
    "corpus_dir = \"corpus\"\n",
    "doc_map = {fname: os.path.join(corpus_dir, fname) for fname in os.listdir(corpus_dir) if fname.endswith(\".txt\")}\n",
    "\n",
    "# --- Query interface ---\n",
    "query = input(\"Enter your search query: \")\n",
    "print(\"user query:\", query)\n",
    "if query:\n",
    "    # Spell correction\n",
    "    corrected_tokens = correct_query(query, vocab)\n",
    "    corrected_query = \" \".join(corrected_tokens)\n",
    "    print(\"Corrected query:\", corrected_query)\n",
    "\n",
    "    results = search_vsm(corrected_tokens, inverted_index, doc_lengths, top_k=10)\n",
    "\n",
    "    display(Markdown(\"## Top 10 Results\"))\n",
    "    for doc_id, score in results:\n",
    "        link = doc_map.get(doc_id, None)\n",
    "        if link:\n",
    "            display(Markdown(f\"**[{doc_id}]({link})** — Score: `{score:.4f}`\"))\n",
    "        else:\n",
    "            display(Markdown(f\"**{doc_id}** — Score: `{score:.4f}` (no file found)\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
