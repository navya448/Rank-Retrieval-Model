{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f36ca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: jellyfish in c:\\python312\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: pyspellchecker in c:\\python312\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\charv\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk jellyfish pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a3623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing file: Adobe.txt\n",
      "\n",
      " Original text snippet: what is adobe?\n",
      "\n",
      "The company was founded in 1982 by John Warn ...\n",
      "Tokens: ['what', 'is', 'adobe', '?', 'the', 'company', 'was', 'founded', 'in', '1982', 'by', 'john', 'warnock', 'and', 'charles']\n",
      " After stopword removal: ['adobe', 'company', 'founded', '1982', 'john', 'warnock', 'charles', 'geschke', 'employed', 'xerox', 'corporation', 'palo', 'alto', 'california', 'research']\n",
      "  After stemming: ['adob', 'compani', 'found', '1982', 'john', 'warnock', 'charl', 'geschk', 'employ', 'xerox', 'corpor', 'palo', 'alto', 'california', 'research']\n",
      "  Example Soundex: ['A310', 'C515', 'F530', '1000', 'J500', 'W652', 'C640', 'G200', 'E514', 'X620', 'C616', 'P400', 'A430', 'C416', 'R262']\n",
      "\n",
      " Processing file: Amazon.txt\n",
      "\n",
      " Original text snippet: What is amazon?\n",
      "\n",
      "Amazon.com, online retailer, manufacturer o ...\n",
      "Tokens: ['what', 'is', 'amazon', '?', 'amazon.com', ',', 'online', 'retailer', ',', 'manufacturer', 'of', 'electronic', 'book', 'readers', ',']\n",
      " After stopword removal: ['amazon', 'online', 'retailer', 'manufacturer', 'electronic', 'book', 'readers', 'web', 'services', 'provider', 'became', 'iconic', 'example', 'electronic', 'commerce']\n",
      "  After stemming: ['amazon', 'onlin', 'retail', 'manufactur', 'electron', 'book', 'reader', 'web', 'servic', 'provid', 'becam', 'icon', 'exampl', 'electron', 'commerc']\n",
      "  Example Soundex: ['A525', 'O545', 'R340', 'M512', 'E423', 'B200', 'R360', 'W100', 'S612', 'P613', 'B250', 'I250', 'E251', 'E423', 'C562']\n",
      "\n",
      " Processing file: apple.txt\n",
      "\n",
      " Original text snippet: what is apple?\n",
      "\n",
      "Apple Inc. is an American multinational tech ...\n",
      "Tokens: ['what', 'is', 'apple', '?', 'apple', 'inc.', 'is', 'an', 'american', 'multinational', 'technology', 'company', 'that', 'specializes', 'in']\n",
      " After stopword removal: ['apple', 'apple', 'american', 'multinational', 'technology', 'company', 'specializes', 'consumer', 'electronics', 'computer', 'software', 'online', 'services', 'apple', 'world']\n",
      "  After stemming: ['appl', 'appl', 'american', 'multin', 'technolog', 'compani', 'special', 'consum', 'electron', 'comput', 'softwar', 'onlin', 'servic', 'appl', 'world']\n",
      "  Example Soundex: ['A140', 'A140', 'A562', 'M435', 'T254', 'C515', 'S124', 'C525', 'E423', 'C513', 'S136', 'O545', 'S612', 'A140', 'W643']\n",
      "\n",
      " Processing file: Binance.txt\n",
      "\n",
      " Original text snippet: What is binance?\n",
      "\n",
      "The Binance Exchange is a leading cryptocu ...\n",
      "Tokens: ['what', 'is', 'binance', '?', 'the', 'binance', 'exchange', 'is', 'a', 'leading', 'cryptocurrency', 'exchange', 'founded', 'in', '2017']\n",
      " After stopword removal: ['binance', 'binance', 'exchange', 'leading', 'cryptocurrency', 'exchange', 'founded', '2017', 'hong', 'kong', 'features', 'strong', 'focus', 'altcoin', 'trading']\n",
      "  After stemming: ['binanc', 'binanc', 'exchang', 'lead', 'cryptocurr', 'exchang', 'found', '2017', 'hong', 'kong', 'featur', 'strong', 'focu', 'altcoin', 'trade']\n",
      "  Example Soundex: ['B552', 'B552', 'E252', 'L300', 'C613', 'E252', 'F530', '2000', 'H520', 'K520', 'F360', 'S365', 'F200', 'A432', 'T630']\n",
      "\n",
      " Processing file: bing.txt\n",
      "\n",
      " Original text snippet: What Is Bing and How to Use It\n",
      "Google isn't the only search  ...\n",
      "Tokens: ['what', 'is', 'bing', 'and', 'how', 'to', 'use', 'it', 'google', 'is', \"n't\", 'the', 'only', 'search', 'engine']\n",
      " After stopword removal: ['bing', 'use', 'google', 'search', 'engine', 'give', 'microsoft', 'bing', 'try', 'anita', 'george', 'updated', 'june', '28', '2020']\n",
      "  After stemming: ['bing', 'use', 'googl', 'search', 'engin', 'give', 'microsoft', 'bing', 'tri', 'anita', 'georg', 'updat', 'june', '28', '2020']\n",
      "  Example Soundex: ['B520', 'U200', 'G240', 'S620', 'E525', 'G100', 'M262', 'B520', 'T600', 'A530', 'G620', 'U133', 'J500', '2000', '2000']\n",
      "\n",
      " Processing file: blackberry.txt\n",
      "\n",
      " Original text snippet: what is blackberry?\n",
      "\n",
      "BlackBerry is a brand of smartphones, t ...\n",
      "Tokens: ['what', 'is', 'blackberry', '?', 'blackberry', 'is', 'a', 'brand', 'of', 'smartphones', ',', 'tablets', ',', 'and', 'services']\n",
      " After stopword removal: ['blackberry', 'blackberry', 'brand', 'smartphones', 'tablets', 'services', 'originally', 'designed', 'marketed', 'canadian', 'company', 'blackberry', 'limited', 'beginning', '2016']\n",
      "  After stemming: ['blackberri', 'blackberri', 'brand', 'smartphon', 'tablet', 'servic', 'origin', 'design', 'market', 'canadian', 'compani', 'blackberri', 'limit', 'begin', '2016']\n",
      "  Example Soundex: ['B421', 'B421', 'B653', 'S563', 'T143', 'S612', 'O625', 'D225', 'M623', 'C535', 'C515', 'B421', 'L530', 'B250', '2000']\n",
      "\n",
      " Processing file: canva.txt\n",
      "\n",
      " Original text snippet: What is Canva? A guide to the graphic design platform's feat ...\n",
      "Tokens: ['what', 'is', 'canva', '?', 'a', 'guide', 'to', 'the', 'graphic', 'design', 'platform', \"'s\", 'features', 'and', 'capabilities']\n",
      " After stopword removal: ['canva', 'guide', 'graphic', 'design', 'platform', 'features', 'capabilities', 'abigail', 'abesamis', 'demarestsep', '19', '2020', 'ist', 'canva', 'guide']\n",
      "  After stemming: ['canva', 'guid', 'graphic', 'design', 'platform', 'featur', 'capabl', 'abigail', 'abesami', 'demarestsep', '19', '2020', 'ist', 'canva', 'guid']\n",
      "  Example Soundex: ['C510', 'G300', 'G612', 'D225', 'P431', 'F360', 'C114', 'A124', 'A125', 'D562', '1000', '2000', 'I230', 'C510', 'G300']\n",
      "\n",
      " Processing file: Dell.txt\n",
      "\n",
      " Original text snippet: What is dell?\n",
      "\n",
      "The company, first named PC’s Limited, was fo ...\n",
      "Tokens: ['what', 'is', 'dell', '?', 'the', 'company', ',', 'first', 'named', 'pc', '’', 's', 'limited', ',', 'was']\n",
      " After stopword removal: ['dell', 'company', 'first', 'named', 'pc', 'limited', 'founded', '1984', 'american', 'michael', 'dell', 'student', 'university', 'texas', 'austin']\n",
      "  After stemming: ['dell', 'compani', 'first', 'name', 'pc', 'limit', 'found', '1984', 'american', 'michael', 'dell', 'student', 'univers', 'texa', 'austin']\n",
      "  Example Soundex: ['D400', 'C515', 'F623', 'N500', 'P200', 'L530', 'F530', '1000', 'A562', 'M240', 'D400', 'S335', 'U516', 'T200', 'A235']\n",
      "\n",
      " Processing file: Discord.txt\n",
      "\n",
      " Original text snippet: What is discord?\n",
      "\n",
      "Discord is a VoIP, instant messaging and d ...\n",
      "Tokens: ['what', 'is', 'discord', '?', 'discord', 'is', 'a', 'voip', ',', 'instant', 'messaging', 'and', 'digital', 'distribution', 'platform']\n",
      " After stopword removal: ['discord', 'discord', 'voip', 'instant', 'messaging', 'digital', 'distribution', 'platform', 'users', 'communicate', 'voice', 'calls', 'video', 'calls', 'text']\n",
      "  After stemming: ['discord', 'discord', 'voip', 'instant', 'messag', 'digit', 'distribut', 'platform', 'user', 'commun', 'voic', 'call', 'video', 'call', 'text']\n",
      "  Example Soundex: ['D263', 'D263', 'V100', 'I523', 'M220', 'D230', 'D236', 'P431', 'U260', 'C550', 'V200', 'C400', 'V300', 'C400', 'T230']\n",
      "\n",
      " Processing file: flipkart.txt\n",
      "\n",
      " Original text snippet: What is flipkart?\n",
      "\n",
      "On May 9, 2018, Walmart announced that it ...\n",
      "Tokens: ['what', 'is', 'flipkart', '?', 'on', 'may', '9', ',', '2018', ',', 'walmart', 'announced', 'that', 'it', 'bought']\n",
      " After stopword removal: ['flipkart', 'may', '9', '2018', 'walmart', 'announced', 'bought', 'majority', 'stake', '77', 'per', 'cent', 'flipkart', 'making', 'largest']\n",
      "  After stemming: ['flipkart', 'may', '9', '2018', 'walmart', 'announc', 'bought', 'major', 'stake', '77', 'per', 'cent', 'flipkart', 'make', 'largest']\n",
      "  Example Soundex: ['F412', 'M000', '9000', '2000', 'W456', 'A552', 'B230', 'M260', 'S320', '7000', 'P600', 'C530', 'F412', 'M200', 'L622']\n",
      "\n",
      " Processing file: google.txt\n",
      "\n",
      " Original text snippet: Originally known as BackRub. Google is a search engine that  ...\n",
      "Tokens: ['originally', 'known', 'as', 'backrub', '.', 'google', 'is', 'a', 'search', 'engine', 'that', 'started', 'development', 'in', '1996']\n",
      " After stopword removal: ['originally', 'known', 'backrub', 'google', 'search', 'engine', 'started', 'development', '1996', 'sergey', 'brin', 'larry', 'page', 'research', 'project']\n",
      "  After stemming: ['origin', 'known', 'backrub', 'googl', 'search', 'engin', 'start', 'develop', '1996', 'sergey', 'brin', 'larri', 'page', 'research', 'project']\n",
      "  Example Soundex: ['O625', 'K550', 'B261', 'G240', 'S620', 'E525', 'S363', 'D141', '1000', 'S620', 'B650', 'L600', 'P200', 'R262', 'P622']\n",
      "\n",
      " Processing file: HP.txt\n",
      "\n",
      " Original text snippet: What is hp?\n",
      "\n",
      "Hewlett-Packard (HP) was founded in 1939 by Wil ...\n",
      "Tokens: ['what', 'is', 'hp', '?', 'hewlett-packard', '(', 'hp', ')', 'was', 'founded', 'in', '1939', 'by', 'william', 'hewlett']\n",
      " After stopword removal: ['hp', 'hp', 'founded', '1939', 'william', 'hewlett', 'david', 'packard', 'known', 'hardware', 'advancements', 'personal', 'computers', 'printers', 'acquired']\n",
      "  After stemming: ['hp', 'hp', 'found', '1939', 'william', 'hewlett', 'david', 'packard', 'known', 'hardwar', 'advanc', 'person', 'comput', 'printer', 'acquir']\n",
      "  Example Soundex: ['H100', 'H100', 'F530', '1000', 'W450', 'H430', 'D130', 'P263', 'K550', 'H636', 'A315', 'P625', 'C513', 'P653', 'A260']\n",
      "\n",
      " Processing file: huawei.txt\n",
      "\n",
      " Original text snippet: what is huawei?\n",
      "\n",
      "Huawei Technologies Co., Ltd. is a Chinese  ...\n",
      "Tokens: ['what', 'is', 'huawei', '?', 'huawei', 'technologies', 'co.', ',', 'ltd.', 'is', 'a', 'chinese', 'multinational', 'technology', 'company']\n",
      " After stopword removal: ['huawei', 'huawei', 'technologies', 'chinese', 'multinational', 'technology', 'company', 'headquartered', 'shenzhen', 'guangdong', 'designs', 'develops', 'sells', 'telecommunications', 'equipment']\n",
      "  After stemming: ['huawei', 'huawei', 'technolog', 'chines', 'multin', 'technolog', 'compani', 'headquart', 'shenzhen', 'guangdong', 'design', 'develop', 'sell', 'telecommun', 'equip']\n",
      "  Example Soundex: ['H000', 'H000', 'T254', 'C520', 'M435', 'T254', 'C515', 'H326', 'S525', 'G523', 'D225', 'D141', 'S400', 'T425', 'E210']\n",
      "\n",
      " Processing file: instagram.txt\n",
      "\n",
      " Original text snippet: What is instagram?\n",
      "\n",
      "With over a billion registered accounts, ...\n",
      "Tokens: ['what', 'is', 'instagram', '?', 'with', 'over', 'a', 'billion', 'registered', 'accounts', ',', 'instagram', ',', 'which', 'was']\n",
      " After stopword removal: ['instagram', 'billion', 'registered', 'accounts', 'instagram', 'bought', 'facebook', '2012', 'become', 'part', 'daily', 'life', 'seems', 'like', 'everyone']\n",
      "  After stemming: ['instagram', 'billion', 'regist', 'account', 'instagram', 'bought', 'facebook', '2012', 'becom', 'part', 'daili', 'life', 'seem', 'like', 'everyon']\n",
      "  Example Soundex: ['I523', 'B450', 'R223', 'A253', 'I523', 'B230', 'F212', '2000', 'B250', 'P630', 'D400', 'L100', 'S500', 'L200', 'E165']\n",
      "\n",
      " Processing file: Lenovo.txt\n",
      "\n",
      " Original text snippet: What is Lenovo?\n",
      "\n",
      "Lenovo is an international technology compa ...\n",
      "Tokens: ['what', 'is', 'lenovo', '?', 'lenovo', 'is', 'an', 'international', 'technology', 'company', 'that', 'produces', 'hardware', 'and', 'software']\n",
      " After stopword removal: ['lenovo', 'lenovo', 'international', 'technology', 'company', 'produces', 'hardware', 'software', 'solutions', 'lenovo', 'corporation', 'result', 'merger', 'chinese', 'company']\n",
      "  After stemming: ['lenovo', 'lenovo', 'intern', 'technolog', 'compani', 'produc', 'hardwar', 'softwar', 'solut', 'lenovo', 'corpor', 'result', 'merger', 'chines', 'compani']\n",
      "  Example Soundex: ['L510', 'L510', 'I536', 'T254', 'C515', 'P632', 'H636', 'S136', 'S430', 'L510', 'C616', 'R243', 'M626', 'C520', 'C515']\n",
      "\n",
      " Processing file: levis.txt\n",
      "\n",
      " Original text snippet: what is levis?\n",
      "\n",
      "Walter A. Haas, (born May 11, 1889, San Fran ...\n",
      "Tokens: ['what', 'is', 'levis', '?', 'walter', 'a.', 'haas', ',', '(', 'born', 'may', '11', ',', '1889', ',']\n",
      " After stopword removal: ['levis', 'walter', 'haas', 'born', 'may', '11', '1889', 'san', 'francisco', '7', '1979', 'san', 'francisco', 'american', 'business']\n",
      "  After stemming: ['levi', 'walter', 'haa', 'born', 'may', '11', '1889', 'san', 'francisco', '7', '1979', 'san', 'francisco', 'american', 'busi']\n",
      "  Example Soundex: ['L100', 'W436', 'H000', 'B650', 'M000', '1000', '1000', 'S500', 'F652', '7000', '1000', 'S500', 'F652', 'A562', 'B200']\n",
      "\n",
      " Processing file: messenger.txt\n",
      "\n",
      " Original text snippet: what is messenger?\n",
      "\n",
      "Users simply download the app to their m ...\n",
      "Tokens: ['what', 'is', 'messenger', '?', 'users', 'simply', 'download', 'the', 'app', 'to', 'their', 'mobile/tablet', 'device', ';', 'the']\n",
      " After stopword removal: ['messenger', 'users', 'simply', 'download', 'app', 'device', 'app', 'also', 'used', 'desktop', 'users', 'option', 'setting', 'messenger', 'using']\n",
      "  After stemming: ['messeng', 'user', 'simpli', 'download', 'app', 'devic', 'app', 'also', 'use', 'desktop', 'user', 'option', 'set', 'messeng', 'use']\n",
      "  Example Soundex: ['M252', 'U260', 'S514', 'D543', 'A100', 'D120', 'A100', 'A420', 'U200', 'D231', 'U260', 'O135', 'S300', 'M252', 'U200']\n",
      "\n",
      " Processing file: microsoft.txt\n",
      "\n",
      " Original text snippet: what is microsoft?\n",
      "\n",
      "Microsoft Corporation is an American mul ...\n",
      "Tokens: ['what', 'is', 'microsoft', '?', 'microsoft', 'corporation', 'is', 'an', 'american', 'multinational', 'technology', 'corporation', 'which', 'produces', 'computer']\n",
      " After stopword removal: ['microsoft', 'microsoft', 'corporation', 'american', 'multinational', 'technology', 'corporation', 'produces', 'computer', 'software', 'consumer', 'electronics', 'personal', 'computers', 'related']\n",
      "  After stemming: ['microsoft', 'microsoft', 'corpor', 'american', 'multin', 'technolog', 'corpor', 'produc', 'comput', 'softwar', 'consum', 'electron', 'person', 'comput', 'relat']\n",
      "  Example Soundex: ['M262', 'M262', 'C616', 'A562', 'M435', 'T254', 'C616', 'P632', 'C513', 'S136', 'C525', 'E423', 'P625', 'C513', 'R430']\n",
      "\n",
      " Processing file: motorola.txt\n",
      "\n",
      " Original text snippet: what is motorola?\n",
      "\n",
      "Motorola, Inc. was an American multinatio ...\n",
      "Tokens: ['what', 'is', 'motorola', '?', 'motorola', ',', 'inc.', 'was', 'an', 'american', 'multinational', 'telecommunications', 'company', 'based', 'in']\n",
      " After stopword removal: ['motorola', 'motorola', 'american', 'multinational', 'telecommunications', 'company', 'based', 'schaumburg', 'illinois', 'united', 'states', 'lost', 'billion', '2007', '2009']\n",
      "  After stemming: ['motorola', 'motorola', 'american', 'multin', 'telecommun', 'compani', 'base', 'schaumburg', 'illinoi', 'unit', 'state', 'lost', 'billion', '2007', '2009']\n",
      "  Example Soundex: ['M364', 'M364', 'A562', 'M435', 'T425', 'C515', 'B200', 'S516', 'I450', 'U530', 'S330', 'L230', 'B450', '2000', '2000']\n",
      "\n",
      " Processing file: nike.txt\n",
      "\n",
      " Original text snippet: What is nike?\n",
      "\n",
      "Nike is a champion brand builder. Its adverti ...\n",
      "Tokens: ['what', 'is', 'nike', '?', 'nike', 'is', 'a', 'champion', 'brand', 'builder', '.', 'its', 'advertising', 'slogans—', '“']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Charv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Charv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Charv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After stopword removal: ['nike', 'nike', 'champion', 'brand', 'builder', 'advertising', 'bo', 'knows', 'finish', 'line', 'moved', 'beyond', 'advertising', 'popular', 'expression']\n",
      "  After stemming: ['nike', 'nike', 'champion', 'brand', 'builder', 'advertis', 'bo', 'know', 'finish', 'line', 'move', 'beyond', 'advertis', 'popular', 'express']\n",
      "  Example Soundex: ['N200', 'N200', 'C515', 'B653', 'B436', 'A316', 'B000', 'K500', 'F520', 'L500', 'M100', 'B530', 'A316', 'P146', 'E216']\n",
      "\n",
      " Processing file: nokia.txt\n",
      "\n",
      " Original text snippet: what is nokia?\n",
      "\n",
      "Nokia Corporation is a Finnish multinational ...\n",
      "Tokens: ['what', 'is', 'nokia', '?', 'nokia', 'corporation', 'is', 'a', 'finnish', 'multinational', 'telecommunications', ',', 'information', 'technology', ',']\n",
      " After stopword removal: ['nokia', 'nokia', 'corporation', 'finnish', 'multinational', 'telecommunications', 'information', 'technology', 'consumer', 'electronics', 'company', 'founded', 'nokia', 'main', 'headquarters']\n",
      "  After stemming: ['nokia', 'nokia', 'corpor', 'finnish', 'multin', 'telecommun', 'inform', 'technolog', 'consum', 'electron', 'compani', 'found', 'nokia', 'main', 'headquart']\n",
      "  Example Soundex: ['N200', 'N200', 'C616', 'F520', 'M435', 'T425', 'I516', 'T254', 'C525', 'E423', 'C515', 'F530', 'N200', 'M500', 'H326']\n",
      "\n",
      " Processing file: Ola.txt\n",
      "\n",
      " Original text snippet: What is Ola?\n",
      "\n",
      "Ola needs no introduction. The first Indian ca ...\n",
      "Tokens: ['what', 'is', 'ola', '?', 'ola', 'needs', 'no', 'introduction', '.', 'the', 'first', 'indian', 'cab', 'aggregator', 'company']\n",
      " After stopword removal: ['ola', 'ola', 'needs', 'introduction', 'first', 'indian', 'cab', 'aggregator', 'company', 'ola', 'made', 'availing', 'cab', 'services', 'smooth']\n",
      "  After stemming: ['ola', 'ola', 'need', 'introduct', 'first', 'indian', 'cab', 'aggreg', 'compani', 'ola', 'made', 'avail', 'cab', 'servic', 'smooth']\n",
      "  Example Soundex: ['O400', 'O400', 'N300', 'I536', 'F623', 'I535', 'C100', 'A262', 'C515', 'O400', 'M300', 'A140', 'C100', 'S612', 'S530']\n",
      "\n",
      " Processing file: operating.txt\n",
      "\n",
      " Original text snippet: What is operating system?\n",
      "\n",
      "An operating system is the most i ...\n",
      "Tokens: ['what', 'is', 'operating', 'system', '?', 'an', 'operating', 'system', 'is', 'the', 'most', 'important', 'software', 'that', 'runs']\n",
      " After stopword removal: ['operating', 'system', 'operating', 'system', 'important', 'software', 'runs', 'computer', 'manages', 'computer', 'memory', 'processes', 'well', 'software', 'hardware']\n",
      "  After stemming: ['oper', 'system', 'oper', 'system', 'import', 'softwar', 'run', 'comput', 'manag', 'comput', 'memori', 'process', 'well', 'softwar', 'hardwar']\n",
      "  Example Soundex: ['O160', 'S235', 'O160', 'S235', 'I516', 'S136', 'R500', 'C513', 'M520', 'C513', 'M560', 'P622', 'W400', 'S136', 'H636']\n",
      "\n",
      " Processing file: paypal.txt\n",
      "\n",
      " Original text snippet: what is paypal?\n",
      "\n",
      "paypal, American e-commerce company formed  ...\n",
      "Tokens: ['what', 'is', 'paypal', '?', 'paypal', ',', 'american', 'e-commerce', 'company', 'formed', 'in', 'march', '2000', 'that', 'specializes']\n",
      " After stopword removal: ['paypal', 'paypal', 'american', 'company', 'formed', 'march', '2000', 'specializes', 'internet', 'money', 'transfers', 'heavily', 'used', 'internet', 'auction']\n",
      "  After stemming: ['paypal', 'paypal', 'american', 'compani', 'form', 'march', '2000', 'special', 'internet', 'money', 'transfer', 'heavili', 'use', 'internet', 'auction']\n",
      "  Example Soundex: ['P140', 'P140', 'A562', 'C515', 'F650', 'M620', '2000', 'S124', 'I536', 'M500', 'T652', 'H140', 'U200', 'I536', 'A235']\n",
      "\n",
      " Processing file: puma.txt\n",
      "\n",
      " Original text snippet: What is puma?\n",
      "\n",
      "Puma SE is engaged in footwear, apparel, and  ...\n",
      "Tokens: ['what', 'is', 'puma', '?', 'puma', 'se', 'is', 'engaged', 'in', 'footwear', ',', 'apparel', ',', 'and', 'accessories']\n",
      " After stopword removal: ['puma', 'puma', 'se', 'engaged', 'footwear', 'apparel', 'accessories', 'business', 'puma', 'cobra', 'golf', 'brand', 'names', 'footwear', 'company']\n",
      "  After stemming: ['puma', 'puma', 'se', 'engag', 'footwear', 'apparel', 'accessori', 'busi', 'puma', 'cobra', 'golf', 'brand', 'name', 'footwear', 'compani']\n",
      "  Example Soundex: ['P500', 'P500', 'S000', 'E522', 'F360', 'A164', 'A226', 'B200', 'P500', 'C160', 'G410', 'B653', 'N500', 'F360', 'C515']\n",
      "\n",
      " Processing file: reddit.txt\n",
      "\n",
      " Original text snippet: What is reddit?\n",
      "\n",
      "If you spend a lot of time online, chances  ...\n",
      "Tokens: ['what', 'is', 'reddit', '?', 'if', 'you', 'spend', 'a', 'lot', 'of', 'time', 'online', ',', 'chances', 'are']\n",
      " After stopword removal: ['reddit', 'spend', 'lot', 'time', 'online', 'chances', 'heard', 'reddit', 'site', 'bills', 'front', 'page', 'internet', 'empty', 'boast']\n",
      "  After stemming: ['reddit', 'spend', 'lot', 'time', 'onlin', 'chanc', 'heard', 'reddit', 'site', 'bill', 'front', 'page', 'internet', 'empti', 'boast']\n",
      "  Example Soundex: ['R330', 'S153', 'L300', 'T500', 'O545', 'C520', 'H630', 'R330', 'S300', 'B400', 'F653', 'P200', 'I536', 'E513', 'B230']\n",
      "\n",
      " Processing file: reliance.txt\n",
      "\n",
      " Original text snippet: what is reliance?\n",
      "\n",
      "Mukesh ambani, in full Mukesh Dhirubhai a ...\n",
      "Tokens: ['what', 'is', 'reliance', '?', 'mukesh', 'ambani', ',', 'in', 'full', 'mukesh', 'dhirubhai', 'ambani', ',', '(', 'born']\n",
      " After stopword removal: ['reliance', 'mukesh', 'ambani', 'full', 'mukesh', 'dhirubhai', 'ambani', 'born', 'april', '19', '1957', 'aden', 'yemen', 'indian', 'business']\n",
      "  After stemming: ['relianc', 'mukesh', 'ambani', 'full', 'mukesh', 'dhirubhai', 'ambani', 'born', 'april', '19', '1957', 'aden', 'yemen', 'indian', 'busi']\n",
      "  Example Soundex: ['R452', 'M220', 'A515', 'F400', 'M220', 'D610', 'A515', 'B650', 'A164', '1000', '1000', 'A350', 'Y550', 'I535', 'B200']\n",
      "\n",
      " Processing file: samsung.txt\n",
      "\n",
      " Original text snippet: what is samsung?\n",
      "\n",
      "Samsung, South Korean company that is one  ...\n",
      "Tokens: ['what', 'is', 'samsung', '?', 'samsung', ',', 'south', 'korean', 'company', 'that', 'is', 'one', 'of', 'the', 'world']\n",
      " After stopword removal: ['samsung', 'samsung', 'south', 'korean', 'company', 'one', 'world', 'largest', 'producers', 'electronic', 'devices', 'samsung', 'specializes', 'production', 'wide']\n",
      "  After stemming: ['samsung', 'samsung', 'south', 'korean', 'compani', 'one', 'world', 'largest', 'produc', 'electron', 'devic', 'samsung', 'special', 'product', 'wide']\n",
      "  Example Soundex: ['S525', 'S525', 'S300', 'K650', 'C515', 'O500', 'W643', 'L622', 'P632', 'E423', 'D120', 'S525', 'S124', 'P632', 'W300']\n",
      "\n",
      " Processing file: shakespeare.txt\n",
      "\n",
      " Original text snippet: what is shakespeare?\n",
      "\n",
      "William shakespeare, shakespeare also  ...\n",
      "Tokens: ['what', 'is', 'shakespeare', '?', 'william', 'shakespeare', ',', 'shakespeare', 'also', 'spelled', 'shakspere', ',', 'byname', 'bard', 'of']\n",
      " After stopword removal: ['shakespeare', 'william', 'shakespeare', 'shakespeare', 'also', 'spelled', 'shakspere', 'byname', 'bard', 'avon', 'swan', 'avon', 'baptized', 'april', '26']\n",
      "  After stemming: ['shakespear', 'william', 'shakespear', 'shakespear', 'also', 'spell', 'shaksper', 'bynam', 'bard', 'avon', 'swan', 'avon', 'baptiz', 'april', '26']\n",
      "  Example Soundex: ['S221', 'W450', 'S221', 'S221', 'A420', 'S140', 'S216', 'B550', 'B630', 'A150', 'S500', 'A150', 'B132', 'A164', '2000']\n",
      "\n",
      " Processing file: skype.txt\n",
      "\n",
      " Original text snippet: What is skype?\n",
      "\n",
      "Skype is a VoIP service that enables people  ...\n",
      "Tokens: ['what', 'is', 'skype', '?', 'skype', 'is', 'a', 'voip', 'service', 'that', 'enables', 'people', 'to', 'make', 'and']\n",
      " After stopword removal: ['skype', 'skype', 'voip', 'service', 'enables', 'people', 'make', 'receive', 'free', 'voice', 'video', 'calls', 'internet', 'using', 'computer']\n",
      "  After stemming: ['skype', 'skype', 'voip', 'servic', 'enabl', 'peopl', 'make', 'receiv', 'free', 'voic', 'video', 'call', 'internet', 'use', 'comput']\n",
      "  Example Soundex: ['S100', 'S100', 'V100', 'S612', 'E514', 'P140', 'M200', 'R210', 'F600', 'V200', 'V300', 'C400', 'I536', 'U200', 'C513']\n",
      "\n",
      " Processing file: sony.txt\n",
      "\n",
      " Original text snippet: what is sony?\n",
      "\n",
      "Sony, in full Sony Corporation, major Japanes ...\n",
      "Tokens: ['what', 'is', 'sony', '?', 'sony', ',', 'in', 'full', 'sony', 'corporation', ',', 'major', 'japanese', 'manufacturer', 'of']\n",
      " After stopword removal: ['sony', 'sony', 'full', 'sony', 'corporation', 'major', 'japanese', 'manufacturer', 'consumer', 'electronics', 'products', 'also', 'involved', 'films', 'music']\n",
      "  After stemming: ['soni', 'soni', 'full', 'soni', 'corpor', 'major', 'japanes', 'manufactur', 'consum', 'electron', 'product', 'also', 'involv', 'film', 'music']\n",
      "  Example Soundex: ['S500', 'S500', 'F400', 'S500', 'C616', 'M260', 'J152', 'M512', 'C525', 'E423', 'P632', 'A420', 'I514', 'F450', 'M220']\n",
      "\n",
      " Processing file: spotify.txt\n",
      "\n",
      " Original text snippet: what is spotify?\n",
      "\n",
      "Sean Parker, (born December 3, 1979), Amer ...\n",
      "Tokens: ['what', 'is', 'spotify', '?', 'sean', 'parker', ',', '(', 'born', 'december', '3', ',', '1979', ')', ',']\n",
      " After stopword removal: ['spotify', 'sean', 'parker', 'born', 'december', '3', '1979', 'american', 'entrepreneur', 'cofounded', '1999', 'computer', 'service', 'napster', 'first']\n",
      "  After stemming: ['spotifi', 'sean', 'parker', 'born', 'decemb', '3', '1979', 'american', 'entrepreneur', 'cofound', '1999', 'comput', 'servic', 'napster', 'first']\n",
      "  Example Soundex: ['S131', 'S500', 'P626', 'B650', 'D251', '3000', '1000', 'A562', 'E536', 'C153', '1000', 'C513', 'S612', 'N123', 'F623']\n",
      "\n",
      " Processing file: steam.txt\n",
      "\n",
      " Original text snippet: What is steam?\n",
      "\n",
      "Steam is a video game digital distribution s ...\n",
      "Tokens: ['what', 'is', 'steam', '?', 'steam', 'is', 'a', 'video', 'game', 'digital', 'distribution', 'service', 'by', 'valve', '.']\n",
      " After stopword removal: ['steam', 'steam', 'video', 'game', 'digital', 'distribution', 'service', 'valve', 'launched', 'standalone', 'software', 'client', 'september', '2003', 'way']\n",
      "  After stemming: ['steam', 'steam', 'video', 'game', 'digit', 'distribut', 'servic', 'valv', 'launch', 'standalon', 'softwar', 'client', 'septemb', '2003', 'way']\n",
      "  Example Soundex: ['S350', 'S350', 'V300', 'G500', 'D230', 'D236', 'S612', 'V410', 'L520', 'S353', 'S136', 'C453', 'S135', '2000', 'W000']\n",
      "\n",
      " Processing file: swiggy.txt\n",
      "\n",
      " Original text snippet: What Is Swiggy And How It’s Working?\n",
      "\n",
      "Swiggy is one of the t ...\n",
      "Tokens: ['what', 'is', 'swiggy', 'and', 'how', 'it', '’', 's', 'working', '?', 'swiggy', 'is', 'one', 'of', 'the']\n",
      " After stopword removal: ['swiggy', 'working', 'swiggy', 'one', 'topmost', 'players', 'online', 'food', 'ordering', 'delivery', 'industry', 'revolves', 'around', 'world', 'delivery']\n",
      "  After stemming: ['swiggi', 'work', 'swiggi', 'one', 'topmost', 'player', 'onlin', 'food', 'order', 'deliveri', 'industri', 'revolv', 'around', 'world', 'deliveri']\n",
      "  Example Soundex: ['S200', 'W620', 'S200', 'O500', 'T152', 'P460', 'O545', 'F300', 'O636', 'D416', 'I532', 'R141', 'A653', 'W643', 'D416']\n",
      "\n",
      " Processing file: telegram.txt\n",
      "\n",
      " Original text snippet: what is telegram?\n",
      "\n",
      "Telegram is a popular cross-platform mess ...\n",
      "Tokens: ['what', 'is', 'telegram', '?', 'telegram', 'is', 'a', 'popular', 'cross-platform', 'messaging', 'app', 'that', 'is', 'widely', 'used']\n",
      " After stopword removal: ['telegram', 'telegram', 'popular', 'messaging', 'app', 'widely', 'used', 'offers', 'enhanced', 'privacy', 'encryption', 'features', 'well', 'support', 'large']\n",
      "  After stemming: ['telegram', 'telegram', 'popular', 'messag', 'app', 'wide', 'use', 'offer', 'enhanc', 'privaci', 'encrypt', 'featur', 'well', 'support', 'larg']\n",
      "  Example Soundex: ['T426', 'T426', 'P146', 'M220', 'A100', 'W300', 'U200', 'O160', 'E552', 'P612', 'E526', 'F360', 'W400', 'S163', 'L620']\n",
      "\n",
      " Processing file: Uber.txt\n",
      "\n",
      " Original text snippet: All about Uber?\n",
      "\n",
      "From that initial launch in San Francisco,  ...\n",
      "Tokens: ['all', 'about', 'uber', '?', 'from', 'that', 'initial', 'launch', 'in', 'san', 'francisco', ',', 'uber', 'started', 'its']\n",
      " After stopword removal: ['uber', 'initial', 'launch', 'san', 'francisco', 'uber', 'started', 'global', 'expanding', 'new', 'us', 'city', 'every', 'month', 'may']\n",
      "  After stemming: ['uber', 'initi', 'launch', 'san', 'francisco', 'uber', 'start', 'global', 'expand', 'new', 'us', 'citi', 'everi', 'month', 'may']\n",
      "  Example Soundex: ['U160', 'I530', 'L520', 'S500', 'F652', 'U160', 'S363', 'G414', 'E215', 'N000', 'U200', 'C300', 'E160', 'M530', 'M000']\n",
      "\n",
      " Processing file: volkswagen.txt\n",
      "\n",
      " Original text snippet: what is volkswagen?\n",
      "\n",
      "Volkswagen Group, also called Volkswage ...\n",
      "Tokens: ['what', 'is', 'volkswagen', '?', 'volkswagen', 'group', ',', 'also', 'called', 'volkswagen', 'ag', ',', 'major', 'german', 'automobile']\n",
      " After stopword removal: ['volkswagen', 'volkswagen', 'group', 'also', 'called', 'volkswagen', 'ag', 'major', 'german', 'automobile', 'manufacturer', 'founded', 'german', 'government', '1937']\n",
      "  After stemming: ['volkswagen', 'volkswagen', 'group', 'also', 'call', 'volkswagen', 'ag', 'major', 'german', 'automobil', 'manufactur', 'found', 'german', 'govern', '1937']\n",
      "  Example Soundex: ['V422', 'V422', 'G610', 'A420', 'C400', 'V422', 'A200', 'M260', 'G655', 'A351', 'M512', 'F530', 'G655', 'G165', '1000']\n",
      "\n",
      " Processing file: whatsapp.txt\n",
      "\n",
      " Original text snippet: What is whatsapp?\n",
      "\n",
      "Launched in 2009, whatsapp is one of the  ...\n",
      "Tokens: ['what', 'is', 'whatsapp', '?', 'launched', 'in', '2009', ',', 'whatsapp', 'is', 'one', 'of', 'the', 'most', 'popular']\n",
      " After stopword removal: ['whatsapp', 'launched', '2009', 'whatsapp', 'one', 'popular', 'text', 'voice', 'messaging', 'apps', 'free', 'use', 'send', 'messages', 'make']\n",
      "  After stemming: ['whatsapp', 'launch', '2009', 'whatsapp', 'one', 'popular', 'text', 'voic', 'messag', 'app', 'free', 'use', 'send', 'messag', 'make']\n",
      "  Example Soundex: ['W321', 'L520', '2000', 'W321', 'O500', 'P146', 'T230', 'V200', 'M220', 'A100', 'F600', 'U200', 'S530', 'M220', 'M200']\n",
      "\n",
      " Processing file: yahoo.txt\n",
      "\n",
      " Original text snippet: What Is Yahoo?\n",
      "Everything you can do on Yahoo.com\n",
      "by Tim Fis ...\n",
      "Tokens: ['what', 'is', 'yahoo', '?', 'everything', 'you', 'can', 'do', 'on', 'yahoo.com', 'by', 'tim', 'fisher', 'updated', 'on']\n",
      " After stopword removal: ['yahoo', 'everything', 'tim', 'fisher', 'updated', 'december', '20', '2019', 'tweet', 'share', 'email', 'around', 'web', 'browsers', 'cloud']\n",
      "  After stemming: ['yahoo', 'everyth', 'tim', 'fisher', 'updat', 'decemb', '20', '2019', 'tweet', 'share', 'email', 'around', 'web', 'browser', 'cloud']\n",
      "  Example Soundex: ['Y000', 'E163', 'T500', 'F260', 'U133', 'D251', '2000', '2000', 'T300', 'S600', 'E540', 'A653', 'W100', 'B626', 'C430']\n",
      "\n",
      " Processing file: youtube.txt\n",
      "\n",
      " Original text snippet: What is youtube?\n",
      "\n",
      "YouTube is a free video sharing website th ...\n",
      "Tokens: ['what', 'is', 'youtube', '?', 'youtube', 'is', 'a', 'free', 'video', 'sharing', 'website', 'that', 'makes', 'it', 'easy']\n",
      " After stopword removal: ['youtube', 'youtube', 'free', 'video', 'sharing', 'website', 'makes', 'easy', 'watch', 'online', 'videos', 'even', 'create', 'upload', 'videos']\n",
      "  After stemming: ['youtub', 'youtub', 'free', 'video', 'share', 'websit', 'make', 'easi', 'watch', 'onlin', 'video', 'even', 'creat', 'upload', 'video']\n",
      "  Example Soundex: ['Y310', 'Y310', 'F600', 'V300', 'S600', 'W123', 'M200', 'E200', 'W320', 'O545', 'V300', 'E150', 'C630', 'U143', 'V300']\n",
      "\n",
      " Processing file: zomato.txt\n",
      "\n",
      " Original text snippet: If you are a restaurant owner or marketing manager for a res ...\n",
      "Tokens: ['if', 'you', 'are', 'a', 'restaurant', 'owner', 'or', 'marketing', 'manager', 'for', 'a', 'restaurant', ',', 'you', '’']\n",
      " After stopword removal: ['restaurant', 'owner', 'marketing', 'manager', 'restaurant', 'love', 'zomato', 'zomato', 'exactly', 'would', 'love', 'introduce', 'platform', 'set', 'business']\n",
      "  After stemming: ['restaur', 'owner', 'market', 'manag', 'restaur', 'love', 'zomato', 'zomato', 'exactli', 'would', 'love', 'introduc', 'platform', 'set', 'busi']\n",
      "  Example Soundex: ['R236', 'O560', 'M623', 'M520', 'R236', 'L100', 'Z530', 'Z530', 'E223', 'W430', 'L100', 'I536', 'P431', 'S300', 'B200']\n",
      "original query: enviroment protecion\n",
      "corrected query: ['environment', 'protection']\n"
     ]
    }
   ],
   "source": [
    "# importing libraries \n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import jellyfish\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# download nltk data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- TEXT PREPROCESSING PIPELINE ---\n",
    "\n",
    "def preprocess_text(text):\n",
    "    print(\"\\n Original text snippet:\", text[:60], \"...\")\n",
    "    #tokenize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    print(\"Tokens:\", tokens[:15])  # show first 15 tokens\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered = [w for w in tokens if w.isalnum() and w not in stop_words]\n",
    "    print(\" After stopword removal:\", filtered[:15])\n",
    "\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(w) for w in filtered]\n",
    "    print(\"  After stemming:\", stemmed[:15])\n",
    "\n",
    "    # soundex encoding\n",
    "    soundex_codes = [jellyfish.soundex(w) for w in stemmed]\n",
    "    print(\"  Example Soundex:\", soundex_codes[:15])\n",
    "\n",
    "    return stemmed, soundex_codes\n",
    "\n",
    "\n",
    "# --- READ CORPUS FILES ---\n",
    "\n",
    "def read_corpus(folder_path):\n",
    "    corpus = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):   # only text files\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            print(f\"\\n Processing file: {filename}\")\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                processed, soundexed = preprocess_text(text)\n",
    "                corpus[filename] = {\n",
    "                    \"original\": text,\n",
    "                    \"processed\": processed,\n",
    "                    \"soundex\": soundexed\n",
    "                }\n",
    "    return corpus\n",
    "\n",
    "\n",
    "# --- SPELLING CORRECTION FEATURE ---\n",
    "\n",
    "def correct_query(query, vocab):\n",
    "    spell = SpellChecker()\n",
    "    tokens = nltk.word_tokenize(query.lower())\n",
    "    corrected = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word not in vocab:  \n",
    "            suggestion = spell.correction(word)\n",
    "            if suggestion is None:  # fallback using levenshtein distance\n",
    "                # find word in vocab with min distance\n",
    "                min_dist = float(\"inf\")\n",
    "                best = word\n",
    "                for v in vocab:\n",
    "                    d = jellyfish.levenshtein_distance(word, v)\n",
    "                    if d < min_dist:\n",
    "                        min_dist = d\n",
    "                        best = v\n",
    "                corrected.append(best)\n",
    "            else:\n",
    "                corrected.append(suggestion)\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return corrected\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"Corpus\"  # put your folder name here\n",
    "    data = read_corpus(folder)\n",
    "\n",
    "    # build vocab from all processed words\n",
    "    vocab = set()\n",
    "    for doc in data.values():\n",
    "        vocab.update(doc[\"processed\"])\n",
    "\n",
    "    # testing query correction\n",
    "    query = \"enviroment protecion\"\n",
    "    print(\"original query:\", query)\n",
    "    print(\"corrected query:\", correct_query(query, vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78640ebd-0088-40e1-99b0-8d30157a322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building inverted index from corpus data...\n",
      "\n",
      "=== BUILDING INVERTED INDEX ===\n",
      "Processing Adobe.txt: 309 unique terms\n",
      "Processing Amazon.txt: 292 unique terms\n",
      "Processing apple.txt: 225 unique terms\n",
      "Processing Binance.txt: 244 unique terms\n",
      "Processing bing.txt: 262 unique terms\n",
      "Processing blackberry.txt: 344 unique terms\n",
      "Processing canva.txt: 185 unique terms\n",
      "Processing Dell.txt: 405 unique terms\n",
      "Processing Discord.txt: 244 unique terms\n",
      "Processing flipkart.txt: 238 unique terms\n",
      "Processing google.txt: 511 unique terms\n",
      "Processing HP.txt: 282 unique terms\n",
      "Processing huawei.txt: 271 unique terms\n",
      "Processing instagram.txt: 247 unique terms\n",
      "Processing Lenovo.txt: 237 unique terms\n",
      "Processing levis.txt: 219 unique terms\n",
      "Processing messenger.txt: 285 unique terms\n",
      "Processing microsoft.txt: 358 unique terms\n",
      "Processing motorola.txt: 348 unique terms\n",
      "Processing nike.txt: 370 unique terms\n",
      "Processing nokia.txt: 242 unique terms\n",
      "Processing Ola.txt: 375 unique terms\n",
      "Processing operating.txt: 243 unique terms\n",
      "Processing paypal.txt: 240 unique terms\n",
      "Processing puma.txt: 233 unique terms\n",
      "Processing reddit.txt: 365 unique terms\n",
      "Processing reliance.txt: 309 unique terms\n",
      "Processing samsung.txt: 192 unique terms\n",
      "Processing shakespeare.txt: 369 unique terms\n",
      "Processing skype.txt: 208 unique terms\n",
      "Processing sony.txt: 390 unique terms\n",
      "Processing spotify.txt: 222 unique terms\n",
      "Processing steam.txt: 259 unique terms\n",
      "Processing swiggy.txt: 190 unique terms\n",
      "Processing telegram.txt: 236 unique terms\n",
      "Processing Uber.txt: 270 unique terms\n",
      "Processing volkswagen.txt: 258 unique terms\n",
      "Processing whatsapp.txt: 317 unique terms\n",
      "Processing yahoo.txt: 189 unique terms\n",
      "Processing youtube.txt: 287 unique terms\n",
      "Processing zomato.txt: 433 unique terms\n",
      "Index built with 3966 unique terms\n",
      "\n",
      "=== INDEX STATISTICS ===\n",
      "Total unique terms: 3966\n",
      "\n",
      "Top 10 terms by document frequency:\n",
      "  also: appears in 35 documents\n",
      "  one: appears in 34 documents\n",
      "  compani: appears in 30 documents\n",
      "  servic: appears in 30 documents\n",
      "  busi: appears in 29 documents\n",
      "  market: appears in 28 documents\n",
      "  includ: appears in 28 documents\n",
      "  new: appears in 28 documents\n",
      "  world: appears in 28 documents\n",
      "  use: appears in 28 documents\n",
      "\n",
      "Sample postings (first 3 terms):\n",
      "  adob:\n",
      "    df: 1\n",
      "    postings: [('Adobe.txt', 21)]\n",
      "  compani:\n",
      "    df: 30\n",
      "    postings: [('Adobe.txt', 6), ('Amazon.txt', 15), ('apple.txt', 10), ('Binance.txt', 2), ('blackberry.txt', 3), ('Dell.txt', 19), ('flipkart.txt', 6), ('google.txt', 4), ('HP.txt', 8), ('huawei.txt', 9), ('Lenovo.txt', 9), ('levis.txt', 9), ('messenger.txt', 1), ('microsoft.txt', 12), ('motorola.txt', 23), ('nike.txt', 13), ('nokia.txt', 18), ('Ola.txt', 7), ('operating.txt', 1), ('paypal.txt', 7), ('puma.txt', 15), ('reliance.txt', 6), ('samsung.txt', 11), ('sony.txt', 20), ('spotify.txt', 3), ('telegram.txt', 2), ('Uber.txt', 8), ('volkswagen.txt', 8), ('yahoo.txt', 1), ('youtube.txt', 3)]\n",
      "  found:\n",
      "    df: 22\n",
      "    postings: [('Adobe.txt', 1), ('apple.txt', 5), ('Binance.txt', 2), ('blackberry.txt', 2), ('Dell.txt', 3), ('HP.txt', 2), ('huawei.txt', 1), ('Lenovo.txt', 2), ('microsoft.txt', 3), ('motorola.txt', 2), ('nokia.txt', 3), ('puma.txt', 3), ('reliance.txt', 1), ('samsung.txt', 1), ('shakespeare.txt', 1), ('spotify.txt', 3), ('steam.txt', 1), ('telegram.txt', 1), ('Uber.txt', 1), ('volkswagen.txt', 2), ('youtube.txt', 1), ('zomato.txt', 1)]\n",
      "\n",
      "=== SAVING INDEX TO inverted_index.json ===\n",
      "Index successfully saved to inverted_index.json\n",
      "File size: 841.69 KB\n",
      "\n",
      "=== SAVING INDEX TO inverted_index.pkl (PICKLE) ===\n",
      "Index successfully saved to inverted_index.pkl\n",
      "File size: 156.54 KB\n",
      "\n",
      "==================================================\n",
      "TESTING LOAD FUNCTIONALITY\n",
      "\n",
      "=== LOADING INDEX FROM inverted_index.json ===\n",
      "Index successfully loaded from inverted_index.json\n",
      "Loaded 3966 terms\n",
      "Loaded index has 3966 terms\n",
      "\n",
      "==================================================\n",
      "TESTING SEARCH FUNCTIONALITY\n",
      "\n",
      "Term: 'environ'\n",
      "Document frequency: 4\n",
      "Postings: [('HP.txt', 3), ('nike.txt', 1), ('operating.txt', 1), ('steam.txt', 1)]\n",
      "\n",
      "Term: 'protect'\n",
      "Document frequency: 5\n",
      "Postings: [('google.txt', 1), ('paypal.txt', 1), ('telegram.txt', 1), ('volkswagen.txt', 1), ('youtube.txt', 1)]\n",
      "\n",
      "Term: 'research'\n",
      "Document frequency: 11\n",
      "Postings: [('Adobe.txt', 1), ('Amazon.txt', 1), ('bing.txt', 1), ('blackberry.txt', 1), ('Dell.txt', 1), ('google.txt', 1), ('huawei.txt', 1), ('microsoft.txt', 2), ('Ola.txt', 2), ('samsung.txt', 1), ('steam.txt', 3)]\n"
     ]
    }
   ],
   "source": [
    "# --- INVERTED INDEX IMPLEMENTATION ---\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def build_inverted_index(corpus_data):\n",
    "    print(\"\\n=== BUILDING INVERTED INDEX ===\")\n",
    "    \n",
    "    # Initialize the inverted index\n",
    "    inverted_index = defaultdict(lambda: {\"df\": 0, \"postings\": []})\n",
    "    \n",
    "    # Process each document\n",
    "    for doc_id, doc_info in corpus_data.items():\n",
    "        processed_terms = doc_info[\"processed\"]  # Get stemmed tokens\n",
    "        \n",
    "        # Count term frequencies in this document\n",
    "        term_freq = Counter(processed_terms)\n",
    "        \n",
    "        print(f\"Processing {doc_id}: {len(term_freq)} unique terms\")\n",
    "        \n",
    "        # Add to inverted index\n",
    "        for term, freq in term_freq.items():\n",
    "            # Add this document to the term's postings list\n",
    "            inverted_index[term][\"postings\"].append((doc_id, freq))\n",
    "            # Increment document frequency for this term\n",
    "            inverted_index[term][\"df\"] += 1\n",
    "    \n",
    "    # Convert defaultdict to regular dict for cleaner output\n",
    "    inverted_index = dict(inverted_index)\n",
    "    \n",
    "    print(f\"Index built with {len(inverted_index)} unique terms\")\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "def display_index_stats(inverted_index):\n",
    "    \"\"\"Display statistics about the inverted index.\"\"\"\n",
    "    print(\"\\n=== INDEX STATISTICS ===\")\n",
    "    print(f\"Total unique terms: {len(inverted_index)}\")\n",
    "    \n",
    "    # Find terms with highest document frequency\n",
    "    top_terms = sorted(inverted_index.items(), \n",
    "                      key=lambda x: x[1][\"df\"], \n",
    "                      reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nTop 10 terms by document frequency:\")\n",
    "    for term, info in top_terms:\n",
    "        print(f\"  {term}: appears in {info['df']} documents\")\n",
    "    \n",
    "    # Sample postings for first few terms\n",
    "    print(\"\\nSample postings (first 3 terms):\")\n",
    "    for i, (term, info) in enumerate(list(inverted_index.items())[:3]):\n",
    "        print(f\"  {term}:\")\n",
    "        print(f\"    df: {info['df']}\")\n",
    "        print(f\"    postings: {info['postings']}\")\n",
    "\n",
    "\n",
    "def save_index_json(inverted_index, filename=\"inverted_index.json\"):\n",
    "    print(f\"\\n=== SAVING INDEX TO {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(inverted_index, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Index successfully saved to {filename}\")\n",
    "        \n",
    "        # Display file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"File size: {file_size:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving index: {e}\")\n",
    "\n",
    "\n",
    "def load_index_json(filename=\"inverted_index.json\"):\n",
    "    \"\"\"\n",
    "    Load inverted index from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Input filename\n",
    "        \n",
    "    Returns:\n",
    "        inverted_index: Loaded index dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== LOADING INDEX FROM {filename} ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            inverted_index = json.load(f)\n",
    "        \n",
    "        print(f\"Index successfully loaded from {filename}\")\n",
    "        print(f\"Loaded {len(inverted_index)} terms\")\n",
    "        return inverted_index\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_index_pickle(inverted_index, filename=\"inverted_index.pkl\"):\n",
    "    \"\"\"\n",
    "    Save inverted index to pickle file (more efficient for large indices).\n",
    "    \n",
    "    Args:\n",
    "        inverted_index: The index dictionary\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== SAVING INDEX TO {filename} (PICKLE) ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(inverted_index, f)\n",
    "        print(f\"Index successfully saved to {filename}\")\n",
    "        \n",
    "        # Display file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename) / 1024  # KB\n",
    "        print(f\"File size: {file_size:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving index: {e}\")\n",
    "\n",
    "\n",
    "def load_index_pickle(filename=\"inverted_index.pkl\"):\n",
    "    \"\"\"\n",
    "    Load inverted index from pickle file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Input filename\n",
    "        \n",
    "    Returns:\n",
    "        inverted_index: Loaded index dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== LOADING INDEX FROM {filename} (PICKLE) ===\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            inverted_index = pickle.load(f)\n",
    "        \n",
    "        print(f\"Index successfully loaded from {filename}\")\n",
    "        print(f\"Loaded {len(inverted_index)} terms\")\n",
    "        return inverted_index\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def search_term_in_index(inverted_index, term):\n",
    "    \"\"\"\n",
    "    Search for a term in the inverted index.\n",
    "    \n",
    "    Args:\n",
    "        inverted_index: The index dictionary\n",
    "        term: Term to search for\n",
    "    \"\"\"\n",
    "    if term in inverted_index:\n",
    "        info = inverted_index[term]\n",
    "        print(f\"\\nTerm: '{term}'\")\n",
    "        print(f\"Document frequency: {info['df']}\")\n",
    "        print(f\"Postings: {info['postings']}\")\n",
    "    else:\n",
    "        print(f\"\\nTerm '{term}' not found in index\")\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION CODE ---\n",
    "if __name__ == \"__main__\":\n",
    "    # This assumes you already have the corpus data from the preprocessing step\n",
    "    # If running separately, uncomment the lines below:\n",
    "    \n",
    "    # folder = \"Corpus\"\n",
    "    # corpus_data = read_corpus(folder)\n",
    "    \n",
    "    # Build the inverted index\n",
    "    print(\"Building inverted index from corpus data...\")\n",
    "    inverted_index = build_inverted_index(data)  # 'data' from preprocessing step\n",
    "    \n",
    "    # Display statistics\n",
    "    display_index_stats(inverted_index)\n",
    "    \n",
    "    # Save the index (both formats)\n",
    "    save_index_json(inverted_index)\n",
    "    save_index_pickle(inverted_index)\n",
    "    \n",
    "    # Test loading\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING LOAD FUNCTIONALITY\")\n",
    "    \n",
    "    loaded_index = load_index_json()\n",
    "    if loaded_index:\n",
    "        print(f\"Loaded index has {len(loaded_index)} terms\")\n",
    "    \n",
    "    # Test search functionality\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING SEARCH FUNCTIONALITY\")\n",
    "    \n",
    "    # Search for some terms (replace with actual terms from your corpus)\n",
    "    test_terms = [\"environ\", \"protect\", \"research\"]  # Example stemmed terms\n",
    "    for term in test_terms:\n",
    "        search_term_in_index(inverted_index, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406c1a-bec2-4b79-befb-85f7d643d77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector lengths saved to doc_lengths.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "def load_index_json(filename=\"inverted_index.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def compute_doc_vector_lengths(inverted_index):\n",
    "    \n",
    "    doc_vectors = {}  \n",
    "    for term, info in inverted_index.items():\n",
    "        for doc_id, tf in info['postings']:\n",
    "            if doc_id not in doc_vectors:\n",
    "                doc_vectors[doc_id] = {}\n",
    "            weight = 1 + math.log10(tf) if tf > 0 else 0\n",
    "            doc_vectors[doc_id][term] = weight\n",
    "\n",
    "    doc_lengths = {}\n",
    "    for doc_id, vec in doc_vectors.items():\n",
    "        length = math.sqrt(sum(w**2 for w in vec.values()))\n",
    "        doc_lengths[doc_id] = length\n",
    "    return doc_lengths\n",
    "\n",
    "def save_doc_lengths(doc_lengths, filename=\"doc_lengths.json\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(doc_lengths, f, indent=2)\n",
    "\n",
    "inverted_index = load_index_json(\"inverted_index.json\")\n",
    "doc_lengths = compute_doc_vector_lengths(inverted_index)\n",
    "save_doc_lengths(doc_lengths, \"doc_lengths.json\")\n",
    "print(\"Document vector lengths saved to doc_lengths.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "484e1560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Original text snippet: mobile and phone ...\n",
      "Tokens: ['mobile', 'and', 'phone']\n",
      " After stopword removal: ['mobile', 'phone']\n",
      "  After stemming: ['mobil', 'phone']\n",
      "  Example Soundex: ['M140', 'P500']\n",
      "Top results for query: 'mobile and phone'\n",
      "nokia.txt: 0.1660\n",
      "skype.txt: 0.1429\n",
      "huawei.txt: 0.1223\n",
      "whatsapp.txt: 0.1115\n",
      "messenger.txt: 0.0950\n",
      "operating.txt: 0.0873\n",
      "zomato.txt: 0.0820\n",
      "blackberry.txt: 0.0674\n",
      "google.txt: 0.0601\n",
      "instagram.txt: 0.0587\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_query(query):\n",
    "    stemmed, _ = preprocess_text(query)\n",
    "    return stemmed\n",
    "\n",
    "def load_index_json(filename=\"inverted_index.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_doc_lengths(filename=\"doc_lengths.json\"):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def search_vsm(query, inverted_index, doc_lengths, top_k=10):\n",
    "    N = len(doc_lengths)\n",
    "    query_terms = preprocess_query(query)\n",
    "    tf_query = Counter(query_terms)\n",
    "\n",
    "    # Compute ltc weights for query\n",
    "    query_weights = {}\n",
    "    for term, tf in tf_query.items():\n",
    "        if term in inverted_index:\n",
    "            df = inverted_index[term]['df']\n",
    "            idf = math.log10(N / df) if df > 0 else 0\n",
    "            weight = (1 + math.log10(tf)) * idf\n",
    "            query_weights[term] = weight\n",
    "        else:\n",
    "            query_weights[term] = 0\n",
    "\n",
    "    # Normalize query vector\n",
    "    qlen = math.sqrt(sum(w**2 for w in query_weights.values()))\n",
    "    if qlen > 0:\n",
    "        for term in query_weights:\n",
    "            query_weights[term] /= qlen\n",
    "\n",
    "    # Score each document\n",
    "    scores = {}\n",
    "    for term, q_weight in query_weights.items():\n",
    "        if term in inverted_index:\n",
    "            for doc_id, tf in inverted_index[term]['postings']:\n",
    "                # lnc for document: weight = 1 + log10(tf)\n",
    "                d_weight = 1 + math.log10(tf) if tf > 0 else 0\n",
    "                d_weight /= doc_lengths[doc_id] if doc_lengths[doc_id] > 0 else 1\n",
    "                scores[doc_id] = scores.get(doc_id, 0) + q_weight * d_weight\n",
    "\n",
    "    # Sort by score desc, then doc_id asc\n",
    "    ranked = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# --- Example usage ---\n",
    "# Load index and doc lengths\n",
    "inverted_index = load_index_json(\"inverted_index.json\")\n",
    "doc_lengths = load_doc_lengths(\"doc_lengths.json\")\n",
    "\n",
    "# User input for query\n",
    "query = input(\"Enter your search query: \")\n",
    "results = search_vsm(query, inverted_index, doc_lengths, top_k=10)\n",
    "\n",
    "print(f\"Top results for query: '{query}'\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"{doc_id}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
